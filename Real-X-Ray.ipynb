{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f1bf6d-bb5b-4a79-8301-e510a1a51b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EncoderDecoderCache\n",
    "from tqdm import tqdm  # Import progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed514d6-7e80-4af5-94d2-7d9146160ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Apoorv\n",
      "[nltk_data]     Bagga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3dc9020-d598-4d68-926c-0af6a108fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (224, 224)\n",
    "MODEL_NAME = \"t5-small\"  # Change to a medical-specific model if available\n",
    "MODEL_PATH = \"xray_report_model.pth\"\n",
    "data_path = \"processed_xray_data.csv\"\n",
    "image_folder = \"images/images_normalized\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd7aef6-d857-4a6f-9108-801d7aa53816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5399fd0-d38d-4f1a-991f-e9841e84339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['filename'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        report_text = row['findings'] + \" \" + row['impression']\n",
    "        tokenized_text = self.tokenizer(report_text, padding='max_length', truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        \n",
    "        return image, tokenized_text.input_ids.squeeze(), tokenized_text.attention_mask.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6c998d-371f-4cf8-9f79-65ebd3271501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class XRayReportGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XRayReportGenerator, self).__init__()\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(2048, 512)  # Extract features\n",
    "        self.transformer = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        img_features = self.cnn(images)\n",
    "        encoder_outputs = self.transformer.get_encoder()(inputs_embeds=img_features.unsqueeze(1))\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            decoder_input_ids = input_ids[:, :-1]  # Remove last token for teacher forcing\n",
    "            labels = input_ids[:, 1:].contiguous()  # Shifted target for loss computation\n",
    "            outputs = self.transformer(\n",
    "                input_ids=decoder_input_ids,\n",
    "                attention_mask=attention_mask[:, :-1],\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                labels=labels\n",
    "            )\n",
    "            return outputs.loss, outputs.logits\n",
    "        \n",
    "        # Inference Mode\n",
    "        generated_ids = self.transformer.generate(encoder_outputs=encoder_outputs, max_length=256)\n",
    "        return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b58167-9859-47d6-a92e-8049689d0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "dataset = XRayDataset(df, image_folder, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9826a49-8e17-4c03-9afc-0cd1676a73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apoorv Bagga\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Apoorv Bagga\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = XRayReportGenerator().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb51ebdb-da4e-4c02-8d28-e41d1d46e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Training model...\")\n",
    "    EPOCHS = 5\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")  # Add progress bar\n",
    "        for images, input_ids, attention_mask in progress_bar:\n",
    "            images, input_ids, attention_mask = images.to(DEVICE), input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(images, input_ids, attention_mask)  # Extract logits correctly\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())  # Live loss update\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Avg Loss: {avg_loss:.4f}\", flush=True)  # Ensure immediate print\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    print(\"Model saved successfully!\")\n",
    "else:\n",
    "    print(\"Loading trained model...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ea4c09-6344-4b6a-91ad-50002013f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Inference\n",
    "def generate_report(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(\"Error: Image file not found!\", flush=True)\n",
    "        return\n",
    "    \n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.forward(image)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    report = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"Generated Report:\", report, flush=True)\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "149883cf-7be4-4a03-a64a-135e1eb1c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Report: heart is normal in size. The mediastinum is unremarkable. The lungs are clear. No acute disease.\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "sample_image_path = \"images/images_normalized/1000_IM-0003-2001.dcm.png\" # Change to actual image path\n",
    "if os.path.exists(sample_image_path):\n",
    "    generate_report(sample_image_path)\n",
    "else:\n",
    "    print(\"Error: Sample image not found. Please provide a valid image path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978f563e-32f0-47a0-99b9-b3cd8e829c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1000_IM-0003-1001.dcm.png', '1000_IM-0003-2001.dcm.png', '1000_IM-0003-3001.dcm.png', '1001_IM-0004-1001.dcm.png', '1001_IM-0004-1002.dcm.png', '1002_IM-0004-1001.dcm.png', '1002_IM-0004-2001.dcm.png', '1003_IM-0005-2002.dcm.png', '1004_IM-0005-1001.dcm.png', '1004_IM-0005-2001.dcm.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"images/images_normalized\")[:10])  # Show first 10 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d86b7b-ee0e-4f43-a22a-58b3d70254ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-1: 0.7096\n",
      "BLEU-2: 0.5390\n",
      "BLEU-3: 0.4296\n",
      "BLEU-4: 0.3225\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "reference_reports = [\n",
    "    \"The lungs are clear.\",\n",
    "    \"Mild opacity in the right lower lobe.\",\n",
    "    \"No signs of pneumonia.\"\n",
    "]\n",
    "\n",
    "generated_reports = [\n",
    "    \"Lungs are clear.\",\n",
    "    \"Right lower lobe shows mild opacity.\",\n",
    "    \"No pneumonia detected.\"\n",
    "]\n",
    "all_references = [[word_tokenize(ref.lower())] for ref in reference_reports]\n",
    "all_hypotheses = [word_tokenize(hyp.lower()) for hyp in generated_reports]\n",
    "\n",
    "smoothie = SmoothingFunction().method4  # Helps with short sentences or no matches\n",
    "\n",
    "# BLEU-1: weights = (1.0, 0, 0, 0)\n",
    "bleu1 = corpus_bleu(all_references, all_hypotheses, weights=(1.0, 0, 0, 0), smoothing_function=smoothie)\n",
    "\n",
    "# BLEU-2: weights = (0.5, 0.5, 0, 0)\n",
    "bleu2 = corpus_bleu(all_references, all_hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "\n",
    "# BLEU-3: weights = (0.33, 0.33, 0.33, 0)\n",
    "bleu3 = corpus_bleu(all_references, all_hypotheses, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothie)\n",
    "\n",
    "# BLEU-4: weights = (0.25, 0.25, 0.25, 0.25)\n",
    "bleu4 = corpus_bleu(all_references, all_hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nBLEU-1: {bleu1:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2:.4f}\")\n",
    "print(f\"BLEU-3: {bleu3:.4f}\")\n",
    "print(f\"BLEU-4: {bleu4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c783c9d-68bf-4dda-9ae8-a94ee5823ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
